# Render Blueprint for CraigLeads Pro Backend
# Deploy with: https://render.com

services:
  # Main FastAPI application
  - type: web
    name: craigleads-backend
    env: python
    region: oregon
    plan: starter
    branch: main
    buildCommand: pip install --no-cache-dir -r requirements.txt
    startCommand: uvicorn app.main:app --host 0.0.0.0 --port $PORT --workers 4
    healthCheckPath: /api/v1/health

    envVars:
      # Application settings
      - key: ENVIRONMENT
        value: production
      - key: DEBUG
        value: false
      - key: LOG_LEVEL
        value: INFO
      - key: API_V1_STR
        value: /api/v1
      - key: PROJECT_NAME
        value: CraigLeads Pro

      # Database (from Render PostgreSQL)
      - key: DATABASE_URL
        fromDatabase:
          name: craigleads-db
          property: connectionString
      - key: DATABASE_POOL_SIZE
        value: 20
      - key: DATABASE_MAX_OVERFLOW
        value: 30

      # Redis (from Render Redis)
      - key: REDIS_URL
        fromService:
          type: redis
          name: craigleads-redis
          property: connectionString

      # Security (set in Render dashboard)
      - key: SECRET_KEY
        generateValue: true
      - key: ACCESS_TOKEN_EXPIRE_MINUTES
        value: 30

      # CORS - Update with your Netlify domain
      - key: ALLOWED_HOSTS
        value: '["https://your-app.netlify.app", "https://*.netlify.app"]'

      # Scraping configuration
      - key: SCRAPER_DELAY_MIN
        value: 1.0
      - key: SCRAPER_DELAY_MAX
        value: 3.0
      - key: SCRAPER_CONCURRENT_LIMIT
        value: 5

      # Rate limiting
      - key: RATE_LIMIT_PER_MINUTE
        value: 60

      # External API keys (set these in Render dashboard)
      - key: OPENAI_API_KEY
        sync: false
      - key: LINKEDIN_API_KEY
        sync: false
      - key: SMTP_USERNAME
        sync: false
      - key: SMTP_PASSWORD
        sync: false
      - key: POSTMARK_SERVER_TOKEN
        sync: false
      - key: TWOCAPTCHA_API_KEY
        sync: false

    # Automatic deploys from GitHub
    autoDeploy: true

    # Disk storage for exports, logs, etc.
    disk:
      name: craigleads-storage
      mountPath: /app/storage
      sizeGB: 1

  # Celery worker for background tasks (optional)
  - type: worker
    name: craigleads-worker
    env: python
    region: oregon
    plan: starter
    branch: main
    buildCommand: pip install --no-cache-dir -r requirements.txt
    startCommand: celery -A celery_app worker --loglevel=info --concurrency=4

    # Shares same environment variables as web service
    envVars:
      # Reference from web service or define separately
      - key: DATABASE_URL
        fromDatabase:
          name: craigleads-db
          property: connectionString
      - key: REDIS_URL
        fromService:
          type: redis
          name: craigleads-redis
          property: connectionString

  # Celery Beat for scheduled tasks (optional)
  - type: worker
    name: craigleads-beat
    env: python
    region: oregon
    plan: starter
    branch: main
    buildCommand: pip install --no-cache-dir -r requirements.txt
    startCommand: celery -A celery_app beat --loglevel=info

    envVars:
      - key: DATABASE_URL
        fromDatabase:
          name: craigleads-db
          property: connectionString
      - key: REDIS_URL
        fromService:
          type: redis
          name: craigleads-redis
          property: connectionString

# Database
databases:
  - name: craigleads-db
    databaseName: craigleads
    plan: starter
    region: oregon
    postgresMajorVersion: 15

    # Enable pgvector extension
    enablePgvector: true

# Redis for caching and Celery
- type: redis
  name: craigleads-redis
  plan: starter
  region: oregon
  maxmemoryPolicy: allkeys-lru
  ipAllowList: []

# Cron jobs (alternative to Celery Beat)
# cronJobs:
#   - name: scrape-scheduler
#     schedule: "0 */6 * * *"  # Every 6 hours
#     command: python -m app.tasks.scheduled_scrape
#
#   - name: cleanup-old-data
#     schedule: "0 2 * * *"  # Daily at 2 AM
#     command: python -m app.tasks.cleanup
